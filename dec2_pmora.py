# -*- coding: utf-8 -*-
"""dec4_full.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKWlxCEXgcqHsjV4AB-yIWqWJp326IlY
"""



############################
# Placido Mora, 2024 (update)

# *** Paper ***
# D. Krishnan, R. Fergus: "Fast Image Deconvolution using
#                          Hyper-Laplacian Priors".
#                          Proceedings of NIPS 2009.

# *** Abstract ***
# The heavy-tailed distribution of gradients in natural scenes have proven effective
# priors for a range of problems such as denoising, deblurring and super-resolution.
# These distributions are well modeled by a hyper-Laplacian p(x) ∝ e^{−k|x|^α},
# typically with 0.5 ≤ α ≤ 0.8. However, the use of sparse distributions makes the
# problem non-convex and impractically slow to solve for multi-megapixel images.
# In this paper we describe a deconvolution approach that is several orders of magnitude
# faster than existing techniques that use hyper-Laplacian priors. We adopt an
# alternating minimization scheme where one of the two phases is a non-convex
# problem that is separable over pixels. This per-pixel sub-problem may be solved
# with a lookup table (LUT). Alternatively, for two specific values of α, 1/2 and 2/3
# an analytic solution can be found, by finding the roots of a cubic and quartic polynomial,
# respectively. Our approach (using either LUTs or analytic formulae) is able to deconvolve
# a 1 megapixel image in less than ∼3 seconds, achieving comparable quality to existing
# methods such as iteratively reweighted least squares (IRLS) that take ∼20 minutes.
# Furthermore, our method is quite general and can easily be extended to related
# image processing problems, beyond the deconvolution application demonstrated.


# *** Notes ***
# 1) This program is an implementation of the paper indicated above. New updates
#    are possible, as new literature on the subject may be available in the future.
#    A short written report with enhanced description of the code is also available (Github).
# 2) All eq.numbers in this code refer to the eq.numbers in the paper and the written report.
# 3) Parameter 'alpha' is assumed 0.5 for this implementation
# 4) The main of the program comprises these steps:
#     a. Test kernel specification
#     b. Input test image selection (examples directly from the web at runtime)
#     c. Image reading
#     d. Image blurrring
#     e. Image processing (decon)



##### LIBRARY IMPORT #################
import cv2
import numpy as np
from scipy import signal
from scipy import misc
import matplotlib.pyplot as plt
import requests
import math
from matplotlib import image as img



##### FUNCTIONS #################
#
# A group of three functions are defined and used in this code. Since they are only three,
# no separate library was implemented for the moment.
# In general, all the function names refer to a particular equation in the original
# paper, for easier reading and understanding of the code.
# They are:
#
#   • 'solve eq3': function for the x sub-problem. This is basically used to solve equation
#     3 of the written report and the original paper.
#   • 'solve eq5 alpha12': function for the w sub-problem. This is basically used to
#     solve equation 5 of the written report and the original paper.
#   • 'solve_fastd': core function for the deconvolution algorithm. It calls the previous two functions (see description)
#


###########################################
def solve_eq3(yi, ki):
  # DESCRIPTION:
  # It solves eq.(3) as described in the paper.
  #
  # INPUT:
  # yi: input noisy/blurry image (grayscale).
  # ki: convolution kernel
  #
  # OUTPUT:
  # tuple [N1,D1,D2]:
  # these are 3 of 4 terms of the solution for eq(3)
  # x = /frac{N1+sigma*N2}{D1+sigma*D2}
  # (as rewritten in the presentation document).
  # the other term (N2) is calculated/updated in
  # the MASTER LOOP of 'solve_fastd'.

  f1= np.array( [[1,-1]] ) #1st-order derivatve filters (page 3)
  f2= np.transpose(f1)
  mm = len(yi)
  nn = len(yi[0])
  N1= np.multiply( np.conj(np.fft.fft2(ki, s=[mm,nn])), np.fft.fft2(yi) )
  D1= np.absolute( np.fft.fft2(ki, s=[mm,nn]) )**2
  D2a=np.absolute( np.fft.fft2(f1,s=[mm,nn]) )**2
  D2b=np.absolute( np.fft.fft2(f2,s=[mm,nn]) )**2
  D2=D2a+D2b
  return N1, D1, D2;



###########################################
def solve_eq5_alpha12(v, beta):
  # DESCRIPTION:
  # It solves eq.(5) as described in the paper.
  #
  # INPUT:
  # v: F^1,F^2 for each pixel i. (see paper)
  # beta: weight updated during optimizing iterations.
  #       The decon solution converges as beta updates (increases).
  #
  # OUTPUT:
  # w: analytical solution to eq(5) for case alpha=1/2.
  #    For such specific case, eq(5) simplifies to eq(10):
  #    w^3 - 2vw^2 + v^2w - sign(v)/4beta^2 = 0
  #    w is the root for this cubic polynomial.

  szv1 = len(v)
  szv2 = len(v[0])
  eps = 1e-6
  kk = -0.25/beta**2
  h = np.multiply( np.ones((szv1,szv2))*kk, np.sign(v) )
  t1 = (2/3)*v
  v2 = np.multiply(v,v)
  v3 = np.multiply(v2,v)
  t2 = np.exp(np.log(-27*h-2*v3 + (3*np.sqrt(3))*np.lib.scimath.sqrt(27*np.power(h,2) + 4*np.multiply(h,v3)) )/3 )
  t3 = np.divide(v2,t2)
  #####
  root = np.zeros( (3,np.size(v[0]),np.size(v[1])), dtype=complex )
  root[0,:,:] = t1 + (2**(1/3))/3*t3 + (t2/(3*2**(1/3)))
  root[1,:,:] = t1 - ((1.0 + 1j*np.sqrt(3))/(3*2**(2/3)))*t3 - ((1.0 - 1j*np.sqrt(3))/(6*2**(1/3)))*t2
  root[2,:,:] = t1 - ((1.0 - 1j*np.sqrt(3))/(3*2**(2/3)))*t3 - ((1.0 + 1j*np.sqrt(3))/(6*2**(1/3)))*t2
  #####
  root[ np.nonzero(np.isnan(root) | np.isinf(root)) ] = 0;
  #####
  newv2 = np.repeat(v[np.newaxis,:,:], 3, axis=0)
  sv2 = np.sign(newv2)
  rsv2 = np.multiply(np.real(root), sv2)
  rf3a=   np.absolute(np.imag(root))
  rf3b= 2*np.absolute(newv2)/3
  rf3c=   np.absolute(newv2)
  rf3d= np.logical_and( (rf3a<eps), (rsv2>rf3b), (rsv2<rf3c) )
  rf3e= np.multiply( rf3d,rsv2 )
  rf3f = np.sort( rf3e, 0, "quick")
  root_flag3 = np.multiply(rf3f,sv2)
  w=root_flag3[1,:,:]
  return w




###########################################
def solve_fastd(yin, k, lam):
  # DESCRIPTION:
  # It solves the decon core problem as described in the paper.
  #
  # INPUT:
  # yin: input noisy/blurry image (grayscale).
  # k:  convolution kernel
  # lam: parameter 'lambda' (balances likelihood and prior-weighting)
  #
  # OUTPUT:
  # yout: output deblurred image (grayscale).

  # -- init -----
  m = len(yin)
  n = len(yin[0])
  yout = yin
  #---
  beta=1
  maxbeta = 2**10
  beta_inc = 2*np.sqrt(2)
  MAX_LOOP2=2
  # -- solve 3 terms of Eq3 paper -----
  N1,D1,D2 = solve_eq3(yin,k)

  # -- gradients -----
  A1= np.diff(yout,1,1)
  A2= np.subtract(yout[:,0],yout[:,n-1])
  A2= A2.reshape(n,1)
  youtx=np.concatenate((A1,A2), axis=1)
  B1= np.diff(yout,1,0)
  B2= np.subtract(yout[0],yout[m-1])
  B2= B2.reshape(1,m)
  youty=np.concatenate((B1,B2))
  # ------

  master_it = 0
  # -- MASTER LOOP -----
  while beta<maxbeta:
    master_it += 1
    print('Master loop, beta =', master_it, beta)
    sigma = beta/lam
    D = np.add(D1,sigma*D2)
    it=0
    for it in range(MAX_LOOP2):
      print('internal loop:', it)
      youtconv = signal.convolve2d(yout, k, boundary='symm', mode='same')
      #- solve eq5 -
      dbvx =  solve_eq5_alpha12(youtx, beta)
      dbvy =  solve_eq5_alpha12(youty, beta)
      #----
      c1= np.subtract(dbvx[:,n-1],dbvx[:,0])
      c1= c1.reshape(n,1)
      c2= -1.0*np.diff(dbvx,1,1)
      c12=np.concatenate((c1,c2), axis=1)
      b1= np.subtract(dbvy[m-1],dbvy[0])
      b1= b1.reshape(1,m)
      b2= -1.0*np.diff(dbvy,1,0)
      b12= c12 + np.concatenate((b1,b2))
      N2 = np.fft.fft2(b12)
      #----
      N = np.add(N1,sigma*N2)
      fyout= np.divide(N,D)
      yout = np.real(np.fft.ifft2(fyout))
      #----
    #end inner
    beta = beta*beta_inc
  #end master
  return yout;





#### PROGRAM START ##############
#MAIN:
if __name__ == '__main__':
  myk=np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
		            [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,.1,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,.5,0,.4,0,.3,0,0,.1,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,.5,0,.6,0,0,.6,.6,0,0,.3,.3,0,.1,0,0,0],
                [0,0,0,0,0,.3,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,.3,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,.1,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,.5,0,.4,0,.3,0,0,.1,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]] )


  #Input images:
  f = open('image1.jpg','wb')
  # hair
  #f.write(requests.get('https://images.pexels.com/photos/1435612/pexels-photo-1435612.jpeg?cs=srgb&dl=pexels-engin-akyurt-1435612.jpg&fm=jpg').content)
  # dog_eye
  f.write(requests.get('https://images.pexels.com/photos/1435517/pexels-photo-1435517.jpeg?cs=srgb&dl=pexels-engin-akyurt-1435517.jpg&fm=jpg').content)
  # cat_fur
  #f.write(requests.get('https://images.pexels.com/photos/162064/cat-british-shorthair-thoroughbred-adidas-162064.jpeg?cs=srgb&dl=pexels-pixabay-162064.jpg&fm=jpg').content)
  # forehead
  #f.write(requests.get('https://pixy.org/src2/647/6474640.jpg').content)
  # eye
  #f.write(requests.get('https://pixy.org/src2/571/5717568.jpg').content)
  # lips
  #https://get.wallhere.com/photo/white-black-women-monochrome-model-portrait-long-hair-sitting-photography-actress-Elizabeth-Henstridge-Person-supermodel-girl-beauty-woman-lady-arm-black-and-white-monochrome-photography-human-positions-portrait-photography-photo-shoot-brown-hair-art-model-4221.jpg
  f.close()

  # Image read
  f0 = img.imread("image1.jpg",)
  rgb_weights = [0.2989, 0.5870, 0.1140]
  f0gray = (np.dot(f0[...,:3], rgb_weights))/255.0


  # Image blur
  IMSIZE = 1024;
  coy = 1872; cox = 1866
  y = f0gray[int(coy-IMSIZE/2):int(coy+IMSIZE/2), int(cox-IMSIZE/2):int(cox+IMSIZE/2)];
  yblur = signal.convolve2d(y, myk, boundary='symm', mode='valid')


  # Image processing (decon)
  landa=7000
  ynice = solve_fastd(yblur, myk, landa)
  plt.figure(1)
  plt.imshow(f0gray,cmap='gray')
  plt.figure(2)
  plt.imshow(yblur,cmap='gray')
  plt.figure(3)
  plt.imshow(ynice,cmap='gray', vmin=0, vmax=1)
  plt.figure(4)
  plt.imshow(myk,cmap='gray')
  print('Main prog END.-')
  # END #